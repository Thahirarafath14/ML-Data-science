# -*- coding: utf-8 -*-
"""Gen+ai_ff_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t3FNhwgV_AVMgHF8Ud_LK4ZXW_cWJLSO
"""

import numpy as np
import pandas as pd
import kagglehub
import os
import matplotlib.pyplot as plt
from sklearn.utils import resample
import re
from tensorflow.keras.preprocessing.text import Tokenizer
import warnings
warnings.filterwarnings('ignore')



#Step -1 load the data
print(f"Amazon fine food reviews data set")
path = kagglehub.dataset_download("snap/amazon-fine-food-reviews")
csv_file = os.path.join(path, "Reviews.csv")
df = pd.read_csv(csv_file)

df.head(2) # 1-2-3-4-5. review-
#df['Score'].value_counts()

#eda  - inves
df.shape
df.columns.to_list()

#EDa
print(f"Total Reviews : {len(df)}")
df.info()
df.isnull().sum()

df_clean = df[['Text',"Score"]].dropna()
df_clean.head(2)
print(f"After cleaning data : {len(df_clean)}") #5l reviews

#5 ratings 1,2,3,4,5
print("RATING DISTRIBUTION")
score_counts = df_clean['Score'].value_counts().sort_index()
print(score_counts)
print(f"Percentages")
for score, count in score_counts.items():
  percentage = (count/len(df_clean))*100
  print(f"{score} star : {percentage:.2f}%")

#1 star, 2 star 3, star
print("Sample review analysis")
for score in [1,2,3,4,5]:
  sample_review = df_clean[df_clean['Score'] == score]['Text'].iloc[0]
  print(f"{score}-star review")
  print(f"{sample_review[:200]}")

#idea senteces are grammaticlly forluate well    some /some
#no words thwet neg.pos could be

#data viz
df_clean['text_length'] = df_clean["Text"].str.len()
df_clean['word_count'] = df_clean['Text'].str.split().str.len()

df_clean

#sizing the canvas
plt.figure(figsize=(18,6))

#plot-1 Rating distribbution
plt.subplot(1,3,1)
score_counts.plot(kind='bar',color='skyblue',alpha=0.8)
plt.title("Rating Distribution",fontsize=14)
plt.xlabel("Rating (Stars)")
plt.xticks(rotation=0)
plt.ylabel("Number of Reviews")
for idx, val in enumerate(score_counts.values):
  plt.text(idx,val+5000, f"{val: }", ha='center',fontweight='bold')

#plot-2 text length dist
plt.subplot(1,3,2)
plt.hist(df_clean['text_length'], bins=50, alpha=0.7, color='lightgreen', edgecolor='red')
plt.title("Text Length Dist",fontweight='bold')
plt.xlabel("Num of char")
plt.ylabel("Freq")
plt.axvline(df_clean['text_length'].mean(), color='red',linestyle="--",
            label=f"Mean: {df_clean['text_length'].mean()}")
plt.legend()

#plot-3 word count dist
plt.subplot(1,3,3)
plt.hist(df_clean['word_count'], bins=50, alpha=0.7, color='lightgreen', edgecolor='red')
plt.title("word count Dist",fontweight='bold')
plt.xlabel("Num of words")
plt.ylabel("Freq")
plt.axvline(df_clean['word_count'].mean(), color='red',linestyle="--",
            label=f"Mean: {df_clean['word_count'].mean()}")
plt.legend()

plt.tight_layout()

# df_clean.head(2)

#convert to binary classification
df_binary = df_clean[df_clean['Score']!= 3].copy() #aprt from 3 al 1,2,4,5
# df_binary['Score'].value_counts()

df_binary['sentiment'] = (df_binary['Score']>=4).astype(int)

df_binary.head(2)
#distrinution. - binary class
print(f"Negative (1-2 star): {sum(df_binary['sentiment'] == 0):,}")
print(f"Positive (4-5 star): {sum(df_binary['sentiment'] == 1):,}")

#check imbalnce - percentage
negative_pct = (sum(df_binary['sentiment'] == 0) / len(df_binary)) * 100
positive_pct = (sum(df_binary['sentiment'] == 1) / len(df_binary)) * 100
print(f"Negative : {negative_pct}")
print(f"Positive : {positive_pct}")

if positive_pct > 70:
  print(f"Dataset is imbalanced!- skewed postive reviews")

#handle imbalnce

#seperations
negative_reviews = df_binary[df_binary['sentiment'] == 0]
positive_reviews = df_binary[df_binary['sentiment'] == 1]

print(f"Original Imbalance")
print(f"Negative : {len(negative_reviews):,} ({len(negative_reviews)/len(df_binary)})")
print(f"Positive : {len(positive_reviews):,} ({len(positive_reviews)/len(df_binary)})")

#[undersmaple majority class] so that match minoroty class
n_minority = len(negative_reviews)

positive_downsampled = resample(positive_reviews,
                                replace=False,  #smaple without replancement
                                n_samples=n_minority, #minoruity
                                random_state=21)

#df_balanced
df_balanced = pd.concat([negative_reviews,positive_downsampled])

#shuffle
df_balanced = df_balanced.sample(frac=1, random_state=21).reset_index(drop=True)

print(f"After balance")
print(f"Total Samples : {len(df_balanced)}")
print(f"Negative : {sum(df_balanced['sentiment'] == 0):,} ({sum(df_balanced['sentiment']==0)/len(df_balanced)**100})")
print(f"Positive : {sum(df_balanced['sentiment'] == 1):,} ({sum(df_balanced['sentiment']==1)/len(df_balanced)**100})")

df_binary.head(2)

df_balanced.head(2)

#take a sample
sample_size = min(50000, len(df_balanced))
df_sample = df_balanced.sample(n=sample_size, random_state=21)

sample_negative = sum(df_sample['sentiment']==0)
sample_positive = sum(df_sample['sentiment']==1)
print(f"sample_negative : {sample_negative}")
print(f"sample_positive : {sample_positive}")

#clean - text -> 1) convert lowercase
                #  2) remove special character :) (U+1F60A).  #, !
                #  3) keep onlu letter and spaces
                #  4) extra whitespcae   i_love__ice-cream
gemini_api_key = "AIzaSyCZGw7XUajKDXpz5OZI7AA9HK0lmxRn4YU"
tavily_api_key = "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"

def clean_text(text):
  text = text.lower()
  #remove all special numbers, keeping onlu letters and spaces
  text = re.sub(r'[^a-zA-Z\s]','', text)
  #romce
  text = ' '.join(text.split())
  return text

print("+++++++Text Preprocessing++++++")
df_balanced['clean_text'] = df_balanced['Text'].apply(clean_text)
df_balanced.head(2)

# ori - clea

df_balanced.shape

#Tokenziation
from tensorflow.keras.preprocessing.sequence import pad_sequences

MAX_FEATURES = 10000 #VOCAB
MAX_LEN = 100 #maximum seq len (num of words per review) len(longhest_seq)

print(f"Max vocab size : {MAX_FEATURES} words")
print(f"Max seq len : {MAX_LEN} words")

X = df_balanced['clean_text'].values #features
y = df_balanced['sentiment'].values #lables 0 -1

tokenizer = Tokenizer(num_words=MAX_FEATURES, oov_token='<OOV>')
tokenizer.fit_on_texts(X)

#
X_sequences = tokenizer.texts_to_sequences(X)

#padding
X_padded = pad_sequences(X_sequences, maxlen=MAX_LEN, padding='post', truncating="post")

print(f"Actual Vocab size : {len(tokenizer.word_index):,} words")
print(f"Final Sequence shape : {X_padded.shape}")

#look at the final data
print(f"ActualOrg text : {X[0][:100]}")
print(f"Org text : {len(X[0][:100])}")
print(f"Sequence : {X_sequences[0][:20]}")
print(f"Padded shape : {X_padded[0].shape}")

len(X[0][:100])

#SPLIT THE DATA
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
   X_padded, y,
   test_size=0.20,
   random_state=42, #where 0-2^32-1
   stratify=y #maintain class balance overall by stratifieng the sample
)

print(f"Training Set : {X_train.shape[0]:,} samples")
print(f"Testing Set : {X_test.shape[0]:,} samples")
print(f"Shape of features : {X_train.shape[1]} (sequ length)")

#balnce
train_pos = sum(y_train)
test_pos = sum(y_test)

print(f"Training - pos : {train_pos:,} ({train_pos/len(y_train)*100})")
print(f"Test - pos : {test_pos:,} ({test_pos/len(y_train)*100})")

#rnn

from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dropout,Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.models import Sequential

def create_rnn_model():
  model = Sequential([
      #word ind to dense vector
      Embedding(input_dim=MAX_FEATURES, output_dim=128, input_length=MAX_LEN),
      SimpleRNN(units=64, return_sequences=False),
      Dropout(0.5),
      Dense(32, activation="relu"),
      Dense(1, activation='sigmoid')
  ])
  return model

rnn_model  = create_rnn_model()

rnn_model.compile(
    optimizer='adam',
    loss="binary_crossentropy",
    metrics=['accuracy']
)

rnn_model.summary()



rnn_history = rnn_model.fit(
    X_train, y_train,
    batch_size=128,
    epochs=5,
    validation_split=0.20,
    verbose=1
)

final_train_acc = rnn_history.history['accuracy'][-1]
print(f"final_train_acc : {final_train_acc}")

import matplotlib.pyplot as plt
plt.figure(figsize=(8,6))
plt.plot(rnn_history.history['loss'], label='Training loss')
plt.plot(rnn_history.history['val_loss'], label='Validation loss')
plt.title("MOdel loss over epoch")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.legend()

import matplotlib.pyplot as plt
plt.figure(figsize=(8,6))
plt.plot(rnn_history.history['accuracy'], label='Training acc')
plt.plot(rnn_history.history['val_accuracy'], label='Validation acc')
plt.title("acc over epoch")
plt.xlabel("epoch")
plt.ylabel("acc")
plt.legend()

#lstm

def create_lstm_model():
  model = Sequential([
      #word ind to dense vector
      Embedding(input_dim=MAX_FEATURES, output_dim=128, input_length=MAX_LEN),

      LSTM(units=64, return_sequences=False),
      Dropout(0.5),
      Dense(32, activation="relu"),
      Dense(1, activation='sigmoid')
  ])
  return model

lstm_model  = create_lstm_model()

lstm_model.compile(
    optimizer='adam',
    loss="binary_crossentropy",
    metrics=['accuracy']
)

lstm_model.summary()

lstm_history = lstm_model.fit(
    X_train, y_train,
    batch_size=128,
    epochs=5,
    validation_split=0.20,
    verbose=1
)

import matplotlib.pyplot as plt
plt.figure(figsize=(4,4))
plt.plot(lstm_history.history['loss'], label='Training loss')
plt.plot(lstm_history.history['val_loss'], label='Validation loss')
plt.title("MOdel loss over epoch")
plt.xlabel("epoch")
plt.ylabel("loss")
plt.legend()

import matplotlib.pyplot as plt
plt.figure(figsize=(8,6))
plt.plot(lstm_history.history['accuracy'], label='Training acc')
plt.plot(lstm_history.history['val_accuracy'], label='Validation acc')
plt.title("acc over epoch")
plt.xlabel("epoch")
plt.ylabel("acc")
plt.legend()

final_train_acc = lstm_history.history['accuracy'][-1]
print(f"final_train_acc : {final_train_acc}")

#hypapar

param_grid = {
    'model_type':['rnn','lstm'],
    'embedding_dim':[64,128],
    'units':[32,64,128],
    'dropout_rate':[0.30,0.50],
    'learning_rate':[0.001,0.01]
}


#search space - random search , gridsearch
for param, values in param_grid.items():
  print(f"{param}:{values}")

total_combinations = 1
for values in param_grid.values():
  total_combinations *= len(values)

print(total_combinations) #2 * 2 * 3 * 2 * 2

#helper function
def create_mode_with_param(model_type='rnn',embedding_dim=128,units=64,dropout_rate=0.5,learning_rate=0.001):
  model=Sequential([
      Embedding(MAX_FEATURES,embedding_dim,input_length=MAX_LEN)
  ])

  if model_type == 'lstm':
    model.add(LSTM(units, return_sequences=False))
  else:
    model.add(SimpleRNN(units, return_sequences=False))

  model.add(Dropout(dropout_rate))
  model.add(Dense(32,activation='relu'))
  model.add(Dense(1, activation='sigmoid'))

  model.compile(
      optimizer = Adam(learning_rate=learning_rate),
      loss="binary_crossentropy",
      metrics=['accuracy']
  )
  return model

import time

def random_search(n_trails=8):
  results = []
  np.random.seed(42)
  for i in range(n_trails):
    params = {
        'model_type': str(np.random.choice(param_grid['model_type'])),
        'embedding_dim': int(np.random.choice(param_grid['embedding_dim'])),
        'units': int(np.random.choice(param_grid['units'])),
        'dropout_rate': float(np.random.choice(param_grid['dropout_rate'])),
        'learning_rate': float(np.random.choice(param_grid['learning_rate'])),
    }

    print(f"Trail {i+1}/{n_trails}")
    print(f"Parameters : {params}")
    try:
      model = create_mode_with_param(**params)
      history = model.fit(
            X_train, y_train,
            batch_size=128,
            epochs=10,  # idea
            validation_split=0.2,
            verbose=1
        )
      val_accuracy = max(history.history['val_accuracy'])
      results.append({
            'trail':i+1,
            'params':params,
            "val_accuracy":val_accuracy,
            "model":model
        })
      print(f"Validation Accuracy. : {val_accuracy}")
    except Exception as e:
      print(f"Error in the trial run {i+1}: {e}")
      continue

  return results

search_results = random_search(8)

#find the bestr para by myself

if search_results:
  best_result = max(search_results, key=lambda x:x['val_accuracy'])

  print(f"Random search results")
  for result in search_results:
    trial = result['trail']
    acc = result['val_accuracy']
    model_type = result['params']['model_type']
    print(f"Trail {trial}: {model_type.upper()} - Accuracy : {acc}\n")

  print(f"Trial : {best_result['trail']}")
  print(f"Parameters : {best_result['params']}") #finer rnn that i ran
  print(f"Validation Acc : {best_result['val_accuracy']}")

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
import pickle

# train my final best model early stopping model cho
if search_results:
  best_params = best_result['params']

  #Step-1 : creating the model
  best_model = create_mode_with_param(**best_params)

  #Step-2 : Setu callback
  callbacks = [
      ModelCheckpoint('best_amazon_sentiment.h5',
                      monitor='val_accuracy',
                      save_best_only = True,
                      mode='max',
                      verbose=1),
      EarlyStopping(
          monitor='val_loss',
          patience=3,
          restore_best_weights=True,
      )
  ]
  final_history = best_model.fit(
      X_train, y_train,
      batch_size=128,
      epochs=3,
      validation_split=0.20,
      callbacks=callbacks
  )

  #save the tokenizer
  with open('best_amazon_sentiment.pkl', "wb") as f:
    pickle.dump(tokenizer, f)

else:
  print("Error")

#load the saved model and run as per the model
from tensorflow.keras.models import load_model

#model
loaded_model = load_model("best_amazon_sentiment.h5")
#loaded tokenizer
with open("best_amazon_sentiment.pkl", "rb") as f:
  loaded_tokenizer = pickle.load(f)

from sklearn.metrics import confusion_matrix, classification_report, accuracy_score

y_pred_proba = loaded_model.predict(X_test)
#tresh 0.5 > < 0.5
y_pred = (y_pred_proba > 0.5).astype(int).flatten()

test_accuracy = accuracy_score(y_test, y_pred)
print(f"test_accuracy : {test_accuracy}")

test_sentences = [
    "the packaging was nice but the product tested was not to my liking",
    "The customer service was excellent. They were helpful and responsive to my questions",
    "I expected my bluetooth headphones to be great but huge dissapointment... noooo bass and little highh!"
]

def predict_sentiment(text):
  clean = clean_text(text)
  #convert to seq
  sequence = loaded_tokenizer.texts_to_sequences([clean]) #[]
  #padding
  padded = pad_sequences(sequence, maxlen=MAX_LEN, padding='post')
  #predict
  prob = loaded_model.predict(padded, verbose=1)[0][0] #this i am not sure
  sentiment = "Positive" if prob > 0.5 else "Negative"
  return sentiment, prob

test_sentences = [
    "the packaging was nice but the product tested was not to my liking",
    "The product was excellent",
    "I expected my bluetooth headphones to be great but huge dissapointment... noooo bass and little highh!",
    "i have observed that the products that i have recieved are as per my preferences and requirements and i can not complain"
]

# test_sentences[1]
predict_sentiment(test_sentences[3])

# for i, sentence in enumerate(test_sentences):
#   sentiment, probability = predict_sentiment(sentence)
#   print(f"Sentence {i+1}: '{sentence}'")
#   print(f"Prediction: {sentiment} (Probability: {probability})")

#Encoder - Decoder (Seq2Seq)

import tensorflow as tf
import numpy as np
from collections import Counter
import random

data = [ ("i am happy","मैं खुश हूँ"),
         ("You are sad","आप दुखी हैं"),
         ("she is tired", "वह थक गया है"),
         ("we are hungry","हम भूखें है"),
         ("they are busy","वे व्यस्त हैं"),
         ("i am cold","मुझे ठंड लग रही है"),
         ("you are late","तुम देरी से आए हो"),
         ("she is happy", "वह खुश है"),
         ("we are ready","हम तैयार हैं")]

# data[0][0]
# data[0][1]

def build_vocab(sentences, lang): #use later
  tokens = Counter()
  for sent in sentences:
    tokens.update(sent.split())
  vocab = {"<PAD>":0, "<SOS>":1, "<EOS>":2}
  for i, token in enumerate(tokens.keys(),3):
    vocab[token] = i
  return vocab

eng_sents = [pair[0] for pair in data]
hin_sent = [pair[1] for pair in data]
eng_vocab = build_vocab(eng_sents, "eng")
hin_vocab = build_vocab(hin_sent, "hin")

print(f"English Vocabulary size : {len(eng_vocab)}")
print(f"Hindi Vocabulary size : {len(hin_vocab)}")

#converting senteces to indices
def sentence_to_indices(sent, vocab):
  indices = [vocab.get(token, vocab.get("<UNK>",0)) for token in sent.split()]
  indices = [vocab['<SOS>']] + indices + [vocab['<EOS>']]
  return indices

#padding - last step  eng- hin ->
def prepare_data(data, eng_vocab, hin_vocab):
  src_data = [sentence_to_indices(pair[0], eng_vocab) for pair in data] #eng
  tgt_data = [sentence_to_indices(pair[1], hin_vocab) for pair in data] #hindi

  #pad sequnces [sos datr eos]
  src_padded = tf.keras.preprocessing.sequence.pad_sequences(
      src_data, padding='post', value=eng_vocab['<PAD>']
  )
  tgt_padded = tf.keras.preprocessing.sequence.pad_sequences(
      tgt_data, padding='post', value=hin_vocab['<PAD>']
  )
  return src_padded, tgt_padded

src_data, tgt_data = prepare_data(data, eng_vocab, hin_vocab)

print(f"Source data shape : {src_data.shape}")
print(f"Target data shape : {tgt_data.shape}")

#Define our Encoder

class Encoder(tf.keras.Model):
  def __init__(self, input_size, embed_size, hidden_size):
    super(Encoder, self).__init__()
    self.embedding = tf.keras.layers.Embedding(input_size, embed_size)
    self.lstm = tf.keras.layers.LSTM(hidden_size, return_state=True)

    #out - hidden state, cell state
  def call(self, x):
    embedded = self.embedding(x)  #batch_size, seq_len, embded size
    _,hidden, cell = self.lstm(embedded)
    return hidden, cell

class Decoder(tf.keras.Model):
  def __init__(self, output_size, embed_size, hidden_size):
    super(Decoder, self).__init__()
    self.embedding = tf.keras.layers.Embedding(output_size, embed_size)
    self.lstm = tf.keras.layers.LSTM(hidden_size, return_state=True, return_sequences=True)
    self.fc = tf.keras.layers.Dense(output_size) #lstm out veco prob

  #   #out - hidden state, cell state
  def call(self, x, hidden , cell):
    embedded = self.embedding(x)  #[#batch_size, seq_len, embded size]
    lstm_out ,hidden, cell = self.lstm(embedded, initial_state=[hidden, cell]) #sew that
    output = self.fc(lstm_out)
    return output, hidden, cell

# combined # Seq2Seq

tgt = [[1,3,4,5,2,0]] #sos-1, eos-2 pad -0

input_token = tgt[0][:3]
input_token

class Seq2Seq(tf.keras.Model):
  def __init__(self, encoder, decoder):
    super(Seq2Seq, self).__init__()
    self.encoder = encoder
    self.decoder = decoder

  def call(self, inputs, training=False, teacher_forcing_ratio=0.5):
    src, tgt = inputs #[[1,4,3,2,6,0]]
    batch_size= tf.shape(src)[0]
    tgt_len = tf.shape(tgt)[1]
    tgt_vocab_size = len(hin_vocab)

    #encode
    hidden, cell = self.encoder(src)

    #empty
    outputs = []

    #start with <SOS>
    input_token = tgt[:,0:1]

    #Decode
    for t in range(1, tgt_len):
      output, hidden, cell = self.decoder(input_token, hidden, cell)
      outputs.append(output)

      #true
      if training:
        teacher_force = random.random() < teacher_forcing_ratio
        input_token = tgt[:, t:t+1] if teacher_force else tf.argmax(output, axis=-1, output_type=tf.int32)
      #false
      else:
        input_token = tf.argmax(output,axis=-1, output_type=tf.int32 )

    if outputs:
      return tf.concat(outputs, axis=1)
    else:
      return tf.zeros((batch_size, 1, tgt_vocab_size))

def train(model, src_data, tgt_data, epochs=100):
  optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)
  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,reduction='none')

  def train_step(src, tgt):
    with tf.GradientTape() as tape:
      outputs = model([src, tgt], training=True, teacher_forcing_ratio=1.0)

      target_lables = tgt[:,1:]

      seq_len = tf.shape(target_lables)[1]
      outputs = outputs[:, :seq_len, :]

      mask = tf.cast(target_lables != hin_vocab["<PAD>"], tf.float32)
      loss = loss_fn(target_lables, outputs)
      loss = loss * mask

      total_loss = tf.reduce_sum(loss)
      total_tokens = tf.reduce_sum(mask)
      mean_loss = total_loss/(total_tokens + 1e-8)

    gradients = tape.gradient(mean_loss, model.trainable_variables) #weights
    optimizer.apply_gradients(zip(gradients, model.trainable_variables))
    return mean_loss

  print("Starting Training....")
  for epoch in range(epochs):
    loss = train_step(src_data, tgt_data)
    if epoch %10 == 0:
      print(f"Epoch {epoch}, Loss : {loss.numpy():.4f}")

  print("Training completed")

input_size = len(eng_vocab)
output_size = len(hin_vocab)
embed_size = 50
hidden_size=100

print(f"Initializing model with :")
print(f"Input vocab size : {input_size}")
print(f"Output vocab size : {output_size}")
print(f"Embedding size : {embed_size}")
print(f"Hidden size (lstm) : {hidden_size}")

encoder = Encoder(input_size, embed_size, hidden_size)
decoder = Decoder(output_size, embed_size, hidden_size)
model = Seq2Seq(encoder, decoder)

dummy_src = tf.zeros((1,5), dtype=tf.int32)
dummy_tgt = tf.zeros((1,5), dtype=tf.int32)
_ = model([dummy_src, dummy_tgt], training=False)

train(model, src_data, tgt_data, epochs=50)

eng_vocab["<SOS>"]

1-week

def translate(model, sentence, eng_vocab, hin_vocab, max_len=15):
  print(f"Translating your sentence ---> : {sentence}")

  tokens = sentence.split()
  indices = [eng_vocab["<SOS>"]] + [eng_vocab.get(token,0) for token in tokens] + [eng_vocab["<EOS>"]]
  print(f"Input tokens :{tokens}")
  print(f"Input Indices : {indices}")

  #convert to tensor
  src_tensor = tf.convert_to_tensor([indices], dtype=tf.int32) #[[]] inpuit out

  #encode
  hidden, cell = model.encoder(src_tensor)
  print(f"Encoded to context vector of shape : {hidden.shape}")

  #docode
  input_token = tf.convert_to_tensor([[hin_vocab['<SOS>']]], dtype=tf.int32)
  output_tokens = []

  print("Decoding Steps: ")
  for step in range(max_len):
    output, hidden, cell = model.decoder(input_token, hidden, cell)
    predicted_token = tf.argmax(output, axis=-1).numpy()[0,0] #btch, seq

    inv_hin_vocab = {v:k for k,v in hin_vocab.items()}
    predicted_word = inv_hin_vocab.get(predicted_token, "<UNK>")

    print(f"  Step {step+1}:{predicted_word} (token {predicted_token})")

    if predicted_token == hin_vocab["<EOS>"]:
      print("   Reached EOS token, stopping the process!")
      break

    output_tokens.append(predicted_token)
    input_token = tf.convert_to_tensor([[predicted_token]], dtype=tf.int32)

  #convert indices to words
  inv_hin_vocab = {v:k for k,v in hin_vocab.items()}
  translated_words = [inv_hin_vocab.get(idx, "<UNK>") for idx in output_tokens]
  translation = " ".join(translated_words)

  print(f"Final Translation : {translation}")
  return translation

test_sentences = ["i am happy", "You are sad", "we are ready","tired is hungry"]

for sentence in test_sentences:
  expected = None
  for eng, hin in data:
    if eng == sentence:
      print(expected)
      expected = hin
      break

  translation = translate(model, sentence, eng_vocab, hin_vocab)
  print(f"Expected : '{expected}'")
  print(f"Got :      '{translation}")
  print("**"*40)

##### transformer - tiny /simple trans

import torch
import torch.nn as nn
import math

#helper modules - way srp

class TinyTransformer(nn.Module):
  def __init__(self,hindi_vocab_size, english_vocab_size, d_model=32):
    super().__init__()
    self.d_model = d_model

    #Embeddings
    self.hindi_emb = nn.Embedding(hindi_vocab_size, d_model)
    self.english_emb = nn.Embedding(english_vocab_size, d_model)

    #Single attention layer
    self.attention = nn.MultiheadAttention(d_model, num_heads=1, batch_first=True)

    #feedforard later
    # hindi - englis
    #inp - 32 op-vocab size -? english _vocab size?
    self.ffn = nn.Linear(d_model, english_vocab_size)

  def forward(self, hindi, english):
    #embed
    h_emb = self.hindi_emb(hindi)
    e_emb = self.english_emb(english)

    #attention                         @query,key,value
    attented, weights = self.attention(e_emb, h_emb, h_emb)

    output = self.ffn(attented)

    return output

pairs = [
         ("मैं खुश हूँ","i am happy"),
         ("आप दुखी हैं","You are sad"),
         ( "वह थक गया है","she is tired"),
         ("हम भूखें है","we are hungry"),
         ("वे व्यस्त हैं","they are busy"),
         ("मुझे ठंड लग रही है","i am cold"),
         ("तुम देरी से आए हो","you are late"),
         ("वह खुश है","she is happy"),
         ("हम तैयार हैं","we are ready")
         ]

#struggle - over a period understand deeper encoder decoder

from sentence_transformers import SentenceTransformer
sentences = ["This is an example sentence", "Each sentence is converted"]

model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
embeddings = model.encode(sentences)
print(embeddings)

# Commented out IPython magic to ensure Python compatibility.
#Namaste Folks, Lets start in 2-4 minutes
!pip install langchain langchain-community pypdf docarray sentence-transformers huggingface_hub llama-cpp-python -q
!apt-get update && apt-get install -y git cmake build-essential -q

# Install llama.cpp
!git clone https://github.com/ggerganov/llama.cpp
# %cd llama.cpp
!make -j$(nproc)
# %cd ..

from huggingface_hub import hf_hub_download

model_path = hf_hub_download(repo_id="bartowski/Llama-3.2-3B-Instruct-GGUF", filename="Llama-3.2-3B-Instruct-Q4_K_M.gguf")

# bartowski/Llama-3.2-3B-Instruct-GGUF

# # Llama-3.2-3B-Instruct-Q4_K_M.gguf

from google.colab import files
uploaded = files.upload()
pdf_path = list(uploaded.keys())[0]

import torch
from langchain_community.llms import LlamaCpp

llm = LlamaCpp(
    model_path=model_path,
    n_ctx = 2048, #context window size 2048 tokens at once prompt +gen
    n_gpu_layers = 33 if torch.cuda.is_available() else 0,
    temperature = 0.7, #0.0 determnistm 0.7 random
    verbose=False #quite
)

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import DocArrayInMemorySearch
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

loader = PyPDFLoader(pdf_path)
pages = loader.load_and_split()

pages

embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

#vector store
vectorstore = DocArrayInMemorySearch.from_documents(pages, embedding=embeddings)
retriever = vectorstore.as_retriever()

#origibal text, embd, meta data

# my name is rudra - [0.12, 0.34,0.98 ....] [fgile_nbame] [size]

retriever

#prompt

#basiz 0shot
template = """Use the following context to answer the question:
{context}

Question : {question}

Answer: """
prompt = PromptTemplate.from_template(template)

#RAG Chain - invoke
chain = (
    {"context":retriever, "question":RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser() # extrat response llm  -> give you in a proper format  indenraiom pace !
)

question = "how to make pizza" #control res
response = chain.invoke(question)

#gaurd rail

print(f"Answer:{response}")

import time

st = time.time()
question = "What is the main topic of the PDF? can you tell me top 2 important points from the document?"
response = chain.invoke(question)
print("Answer:{response}")
ed = time.time()
print(f"Inference is at :{(ed-st):.3f} Seconds")





!pip install google-generativeai tavily-python

"""## Gen Ai - Basic Api Calls"""

import google.generativeai as genai
from tavily import TavilyClient

genai.configure(api_key="AIzaSyCZGw7XUajKDXpz5OZI7AA9HK0lmxRn4YU")
model = genai.GenerativeModel('gemini-2.0-flash')
tavily = TavilyClient(api_key="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")

response = model.generate_content("Provide a very short response, what is ai?")

response.text

search_result = tavily.search("latest AI news", max_results=2)
for result in search_result['results']:
  print(f"Title: {result['title']}")
  print(f"URL : {result['url']}")
  print(f"Content : {result['content'][:100]}...")
  print()

#fact check

import google.generativeai as genai
from tavily import TavilyClient
import json
from datetime import datetime

GENAI_API_KEY = "AIzaSyCZGw7XUajKDXpz5OZI7AA9HK0lmxRn4YU"
genai.configure(api_key=GENAI_API_KEY)
TAVILY_API_KEY = "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"

#initilize the clinets - api module
model = genai.GenerativeModel('gemini-2.0-flash')
#some llm are mulrimodel
tavily_client = TavilyClient(api_key=TAVILY_API_KEY)

def fact_check(claim):
  print(f"Fact-Checking: {claim}")
  print(f"="*60)

  try:
    print(f"Searching for evidence....")
    search_result = tavily_client.search(
        query=claim,
        search_depth = "advanced",
        max_results=5
    )

    if not search_result.get('results'):
      print(f"No search results were found....")


    evidence = ""
    sources = []

    for i, result in enumerate(search_result.get('results',[])[:5]):
      title = result.get('title',"Unknown")
      content = result.get('content',"No content")
      url = result.get('url',"NO URL")

      evidence += f"Source : {i+1}:{title}\n{content[:400]}....\n\n"
      sources.append({'title':title, 'url':url})
    print(f"Found {len(sources)} Sources")

    #Anlaysis with gemini
    print(f"Analyzing claim.....")

    prompt = f"""
    You are a professional fact-checker. Analayze this claim against the evidence:

    CLAIM:{claim}

    EVIDENCE:{evidence}

    Provide a fact-check analysis with:

    1. VERDICT: Choose ONE of these:
        -TRUE: Claim is factually accuract
        -FALSE: Claim is factually incorrect
        -PARTIALLY TRUE: Claim has some triuth but is misleading
        -UNVERIFIED: Insufficient evidence to determine accuracy
        -OUTDATED: Claim was trye but circumstances have changed

    2. CONFIDENCE: High/Medium/Low

    3. EXPLANATION: Clear 2-3 lines sentence explanation of your verdict

    4. KEY EVIDENCE: Brief summary of the most important evidence

    keep it concise and factual
    """

    response = model.generate_content(prompt)
    analysis = response.text

    print(f"FACT CHEK RESULTS....")
    print("="*60)
    print(analysis)

    print(f"SOURCES.....")
    print(f"-"*30)
    for i, source in enumerate(sources,1):
      print(f"{i}. {source['title']}")
      print(f"  {source['url']}")

    print(f"Completed at: {datetime.now()}")
  except Exception as e:
    print(f"Error:{str(e)}")

def main():
  print("SIMPLE FACT CHEKER WITH GEMINI AND TAVILY")
  print("Type 'quit' or exit")
  print("="*60)

  while True:
    try:
      claim = input("\n Enter your claim to fact-check: ").strip()

      if claim.lower() in ['quit','q','exit']:
        print("Goodbye")
        break

      if not claim:
        print(f"Please enter the claim")
        continue

      fact_check(claim)
      print("\n" + "="*60)

    except KeyboardInterrupt:
      print("Goodbye")
      break
    except Exception as e:
      print(f"Unexpected error: {str(e)}")

if __name__ == "__main__":
  main()

#travel buddy

class TravelPlanningAssistant:
  def __init__(self):
    self.destination = ""
    self.duration = ""
    self.budget = ""
    self.interests = [] #array not a str
    self.travel_style = ""

  def get_travel_info(self):
    print(f"Welcome to Travel buddy...")
    print("="*60)

    self.destination = input("Where do you want to go? ").strip()
    self.duration = input("How many days? ").strip()
    self.budget = input("What's your budget range? (500-1000, midrange, luxury)").strip()

    print(f"\nWhat are you interested in? (seperate with commas)")
    print("Examples : food, culture, adventure, nature, shopping, history")
    interests_input = input("Interest: ").strip()
    self.interests = [interest.strip() for interest in interests_input.split(',') if interest.strip()]

    print("\nTravel Style")
    print(f"1. Backpacker/Budget")
    print(f"2. Comfort/Mid-range")
    print(f"3. Luxury")
    print(f"4. Adventure")
    print(f"5. Cultural/Historical")
    style_choice = input("Choose (1-5): ").strip()

    styles= {
        "1":"Backpacker/Budget",
        "2":"Comfort/Mid-range",
        "3":"Luxury",
        "4":"Adventure",
        "5":"Cultural/Historical"
    }

    self.travel_style = styles.get(style_choice,"mid-range")

  def research_desitination(self):
    print(f"\n Researching Destination {self.destination}...")

    research_queries = [
        f"{self.destination} travel guide 2025-2026",
        f"{self.destination} best attractions things to do",
        f"{self.destination} travel safety current situation",
        f"{self.destination} weather climate best time to visit",
        f"{self.destination} budget costs accomodation and food",
    ]

    all_research = {}

    for query in research_queries:
      try:
        print(f"Searching : {query}....")
        results = tavily_client.search(query=query, max_results=3)
        research_text = ""
        for result in results.get('results',[]):
          research_text += f"Sources : {result.get('title','Unknown')}\n"
          research_text += f"{result.get('content','No Content')}\n\n"
        all_research[query] = research_text
      except Exception as e:
        print(f"Error Searching : {query}: {str(e)}")
        all_research[query] = "No information available"
    return all_research

  def create_travel_plan(self, research_data):
    print(f"Creating your personalizd travel plan....")

    combined_research = ""
    for query, content in research_data.items():
      combined_research += f"=== {query} ===\n{content}\n\n"

    prompt = f"""
    You are an expert travel planner. Create a comprehensive and detailed travel plan based on:

    DESTINATION : {self.destination}
    DURATION: {self.duration}
    BUDGET:{self.budget}
    INTERESTS:{self.interests}
    TRAVEL_STYLE:{self.travel_style}

    RESEARCH DATA: {combined_research[:8000]}

    Create a detailed plan with:

    1.**DESTINATION OVERVIEW**
      -Brief description and highlights
      -Best time time to visit
      -Cultural tips and etiquette

    2.**SAFETY & PRACTICAL INFO**
      -Current safety situation
      -Visa Requirements
      -Currency exchange and payment method
      - Language Tips

    3. **DAILY ITENARY**
      -Day-by-day plan for {self.destination}
      -Mix of must-see attractions and personal interests
      -Include travel time between locations

    4.**FOOD AVAILABE**
      -Country specific cusisine information
      -Veg food vs Non-veg food based information for veg or vegal travellers

    5. **EMERGENCY CONTACT**
      -Imporatant phone numbers
      -Embassy/consulate info

    Make it practical, detailed, and personalized to their interests and budget
    """

    try:
      response = model.generate_content(prompt)
      return response.text
    except Exception as e:
      print(f"Error creating travel plan : {str(e)}")

  def get_current_alerts(self):
    print(f"Checking current travel alerts... Travel advisory")
    try:
      alert_query = f"{self.destination} travel advisory warning alert 2025 2026"
      result = tavily_client.search(query=alert_query, max_results=3)

      alerts = ""
      for result in results.get('results',[]):
        alerts += f"⚠️ {result.get('title','Unknown')}\n"
        alerts += f"⚠️ {result.get('content','No Content')[:200]}\n\n"
      return alerts if alerts else "No current travel alerts found!!"
    except Exception as e:
      return f"Eroor checking alerts: {e}"

  def run(self):
    try:
      self.get_travel_info()
      research_data = self.research_desitination()
      alerts = self.get_current_alerts()

      travel_plan = self.create_travel_plan(research_data)

      print("\n" + "="*60)
      print(f"Your personalized travel plan is.....")
      print("="*60)
      print(travel_plan)

      print("\n" + "="*60)
      print(f"Current travel alerts.....")
      print("="*60)
      print(alerts)

      print("\n" + "="*60)
      print(f"Travel Summary ")
      print(f"Destination : {self.destination}")
      print(f"Duration : {self.duration}")
      print(f"Budget : {self.budget}")
      print(f"Interests : {self.interests}")
      print(f"Style : {self.travel_style}")

      save = input("\n Save this plan to file? (y/n): ").strip().lower()
      if save == 'y':
            filename = f"{travel_plan_my_file}.txt"
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(f"Travel plan")
                f.write(travel_plan)
                f.write(alerts)
            print(f"Plan saved")
    except Exception as e:
            pass

def main():
  assistant = TravelPlanningAssistant()

  while True:
    try:
      assistant.run()

      another = input("\n Plan another trip (y/n) : ").strip().lower()
      if another != 'y':
        print(f"Happy travels!")
        break
      assistant = TravelPlanningAssistant()
    except Exception as e:
      print(f"Goodbye")
      break

if __name__ == "__main__":
  main()

gradio